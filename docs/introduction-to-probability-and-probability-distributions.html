<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Introduction to Probability and Probability Distributions | Quantitative Genetics Graduate Education Module - Spring 2021</title>
  <meta name="description" content="This is the book of materials we will be using for the Quantitative Genetics GEM at the University of Oregon for the Spring Term of 2021" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Introduction to Probability and Probability Distributions | Quantitative Genetics Graduate Education Module - Spring 2021" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the book of materials we will be using for the Quantitative Genetics GEM at the University of Oregon for the Spring Term of 2021" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Introduction to Probability and Probability Distributions | Quantitative Genetics Graduate Education Module - Spring 2021" />
  
  <meta name="twitter:description" content="This is the book of materials we will be using for the Quantitative Genetics GEM at the University of Oregon for the Spring Term of 2021" />
  

<meta name="author" content="William A. Cresko" />


<meta name="date" content="2021-04-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="a-brief-introduction-to-rmarkdown.html"/>
<link rel="next" href="correlation-and-simple-linear-regression.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Genetics GEM</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Course Overview</a></li>
<li class="chapter" data-level="2" data-path="introduction-to-the-course.html"><a href="introduction-to-the-course.html"><i class="fa fa-check"></i><b>2</b> Introduction to the course</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-the-course.html"><a href="introduction-to-the-course.html#instructor"><i class="fa fa-check"></i><b>2.1</b> Instructor</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-the-course.html"><a href="introduction-to-the-course.html#course-information"><i class="fa fa-check"></i><b>2.2</b> Course Information</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-the-course.html"><a href="introduction-to-the-course.html#software"><i class="fa fa-check"></i><b>2.3</b> Software</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-the-course.html"><a href="introduction-to-the-course.html#inclusion-and-accessibility"><i class="fa fa-check"></i><b>2.4</b> Inclusion and Accessibility</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="course-schedule.html"><a href="course-schedule.html"><i class="fa fa-check"></i><b>3</b> Course Schedule</a>
<ul>
<li class="chapter" data-level="3.1" data-path="course-schedule.html"><a href="course-schedule.html#background-material"><i class="fa fa-check"></i><b>3.1</b> Background Material</a></li>
<li class="chapter" data-level="3.2" data-path="course-schedule.html"><a href="course-schedule.html#heritability"><i class="fa fa-check"></i><b>3.2</b> Heritability</a></li>
<li class="chapter" data-level="3.3" data-path="course-schedule.html"><a href="course-schedule.html#selection-analysis"><i class="fa fa-check"></i><b>3.3</b> Selection Analysis</a></li>
<li class="chapter" data-level="3.4" data-path="course-schedule.html"><a href="course-schedule.html#quantitative-genetic-mapping"><i class="fa fa-check"></i><b>3.4</b> Quantitative Genetic Mapping</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="getting-started-with-r-and-studio.html"><a href="getting-started-with-r-and-studio.html"><i class="fa fa-check"></i><b>4</b> Getting Started with R and Studio</a>
<ul>
<li class="chapter" data-level="4.1" data-path="getting-started-with-r-and-studio.html"><a href="getting-started-with-r-and-studio.html#introduction-to-r-and-rstudio"><i class="fa fa-check"></i><b>4.1</b> Introduction to R and RStudio</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="getting-started-with-r-and-studio.html"><a href="getting-started-with-r-and-studio.html#learning-resources"><i class="fa fa-check"></i><b>4.1.1</b> Learning resources</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html"><i class="fa fa-check"></i><b>5</b> Organizing and manipulating data files</a>
<ul>
<li class="chapter" data-level="5.1" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html#navigating-file-systems-from-the-command-line"><i class="fa fa-check"></i><b>5.2</b> Navigating file systems from the command line</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html#access-to-the-command-line"><i class="fa fa-check"></i><b>5.2.1</b> Access to the command line</a></li>
<li class="chapter" data-level="5.2.2" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html#navigating-directories-and-files"><i class="fa fa-check"></i><b>5.2.2</b> Navigating directories and files</a></li>
<li class="chapter" data-level="5.2.3" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html#useful-unix-commands-for-file-manipulation"><i class="fa fa-check"></i><b>5.2.3</b> Useful UNIX commands for file manipulation</a></li>
<li class="chapter" data-level="5.2.4" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html#a-quick-word-on-pipes-and-carrots"><i class="fa fa-check"></i><b>5.2.4</b> A quick word on pipes and carrots</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html#data-file-and-data-file-entry-dos-and-donts"><i class="fa fa-check"></i><b>5.3</b> Data file and data file entry dos and don’ts</a></li>
<li class="chapter" data-level="5.4" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html#exercises-associated-with-this-chapter"><i class="fa fa-check"></i><b>5.4</b> Exercises associated with this chapter:</a></li>
<li class="chapter" data-level="5.5" data-path="organizing-and-manipulating-data-files.html"><a href="organizing-and-manipulating-data-files.html#additional-learning-resources"><i class="fa fa-check"></i><b>5.5</b> Additional learning resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="an-introduction-to-the-r-language.html"><a href="an-introduction-to-the-r-language.html"><i class="fa fa-check"></i><b>6</b> An Introduction to the R language</a>
<ul>
<li class="chapter" data-level="6.1" data-path="an-introduction-to-the-r-language.html"><a href="an-introduction-to-the-r-language.html#background"><i class="fa fa-check"></i><b>6.1</b> Background</a></li>
<li class="chapter" data-level="6.2" data-path="an-introduction-to-the-r-language.html"><a href="an-introduction-to-the-r-language.html#why-use-r"><i class="fa fa-check"></i><b>6.2</b> Why use <code>R</code>?</a></li>
<li class="chapter" data-level="6.3" data-path="an-introduction-to-the-r-language.html"><a href="an-introduction-to-the-r-language.html#important-r-terms-and-definitions"><i class="fa fa-check"></i><b>6.3</b> Important <code>R</code> terms and definitions</a></li>
<li class="chapter" data-level="6.4" data-path="an-introduction-to-the-r-language.html"><a href="an-introduction-to-the-r-language.html#getting-started-with-r-via-the-rstudio-environment"><i class="fa fa-check"></i><b>6.4</b> Getting started with <code>R</code> via the RStudio Environment</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="an-introduction-to-the-r-language.html"><a href="an-introduction-to-the-r-language.html#r-programming-basics"><i class="fa fa-check"></i><b>6.4.1</b> R Programming Basics</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="an-introduction-to-the-r-language.html"><a href="an-introduction-to-the-r-language.html#additional-learning-resources-1"><i class="fa fa-check"></i><b>6.5</b> Additional learning resources:</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><i class="fa fa-check"></i><b>7</b> More R Functions, Complex Objects, Basic Plotting, and RMarkdown</a>
<ul>
<li class="chapter" data-level="7.1" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#background-1"><i class="fa fa-check"></i><b>7.1</b> Background</a></li>
<li class="chapter" data-level="7.2" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#more-on-functions"><i class="fa fa-check"></i><b>7.2</b> More on functions</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#more-base-r-functions-useful-for-working-with-vectors"><i class="fa fa-check"></i><b>7.2.1</b> More base <code>R</code> functions useful for working with vectors</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#indexing-vectors"><i class="fa fa-check"></i><b>7.3</b> Indexing vectors</a></li>
<li class="chapter" data-level="7.4" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#more-complex-data-objects-in-r"><i class="fa fa-check"></i><b>7.4</b> More complex data objects in <code>R</code></a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#lists"><i class="fa fa-check"></i><b>7.4.1</b> Lists</a></li>
<li class="chapter" data-level="7.4.2" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#data-frames"><i class="fa fa-check"></i><b>7.4.2</b> Data frames</a></li>
<li class="chapter" data-level="7.4.3" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#matrices"><i class="fa fa-check"></i><b>7.4.3</b> Matrices</a></li>
<li class="chapter" data-level="7.4.4" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#a-few-additional-base-r-functions-for-working-with-complex-r-objects"><i class="fa fa-check"></i><b>7.4.4</b> A few additional base <code>R</code> functions for working with complex <code>R</code> objects</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#some-brief-notes-on-basic-programming-in-r"><i class="fa fa-check"></i><b>7.5</b> Some brief notes on basic programming in <code>R</code></a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#conditional-statements-with-ifelse"><i class="fa fa-check"></i><b>7.5.1</b> conditional statements with <code>ifelse()</code></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#the-split-apply-combine-approach-to-data-analysis"><i class="fa fa-check"></i><b>7.6</b> The Split-Apply-Combine approach to data analysis</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#replicate-apply-tapply-and-aggregate"><i class="fa fa-check"></i><b>7.6.1</b> <code>replicate()</code>, <code>apply()</code>, <code>tapply()</code>, and <code>aggregate()</code></a></li>
<li class="chapter" data-level="7.6.2" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#for-loops-in-r"><i class="fa fa-check"></i><b>7.6.2</b> For loops in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#fundamentals-of-plotting-in-r"><i class="fa fa-check"></i><b>7.7</b> Fundamentals of plotting in <code>R</code></a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#basic-plotting-with-plot"><i class="fa fa-check"></i><b>7.7.1</b> Basic plotting with <code>plot()</code></a></li>
<li class="chapter" data-level="7.7.2" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#histograms-using-hist"><i class="fa fa-check"></i><b>7.7.2</b> Histograms using <code>hist()</code></a></li>
<li class="chapter" data-level="7.7.3" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#boxplots-using-boxplot"><i class="fa fa-check"></i><b>7.7.3</b> Boxplots using <code>boxplot()</code></a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html"><a href="more-r-functions-complex-objects-basic-plotting-and-rmarkdown.html#additional-learning-resources-2"><i class="fa fa-check"></i><b>7.8</b> Additional learning resources:</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="a-brief-introduction-to-rmarkdown.html"><a href="a-brief-introduction-to-rmarkdown.html"><i class="fa fa-check"></i><b>8</b> A brief Introduction to RMarkdown</a>
<ul>
<li class="chapter" data-level="8.1" data-path="a-brief-introduction-to-rmarkdown.html"><a href="a-brief-introduction-to-rmarkdown.html#what-is-rmarkdown"><i class="fa fa-check"></i><b>8.1</b> What is <code>RMarkdown</code></a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="a-brief-introduction-to-rmarkdown.html"><a href="a-brief-introduction-to-rmarkdown.html#rmarkdown-formatting-basics"><i class="fa fa-check"></i><b>8.1.1</b> <code>RMarkdown</code> formatting basics</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="a-brief-introduction-to-rmarkdown.html"><a href="a-brief-introduction-to-rmarkdown.html#experiment-with-headers"><i class="fa fa-check"></i><b>8.2</b> Experiment with headers</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="a-brief-introduction-to-rmarkdown.html"><a href="a-brief-introduction-to-rmarkdown.html#try-a-third-level-header"><i class="fa fa-check"></i><b>8.2.1</b> Try a third-level header</a></li>
<li class="chapter" data-level="8.2.2" data-path="a-brief-introduction-to-rmarkdown.html"><a href="a-brief-introduction-to-rmarkdown.html#rmarkdown-code-chunk-options"><i class="fa fa-check"></i><b>8.2.2</b> <code>RMarkdown</code> code chunk options</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="a-brief-introduction-to-rmarkdown.html"><a href="a-brief-introduction-to-rmarkdown.html#additional-learning-resources-3"><i class="fa fa-check"></i><b>8.3</b> Additional learning resources:</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html"><i class="fa fa-check"></i><b>9</b> Introduction to Probability and Probability Distributions</a>
<ul>
<li class="chapter" data-level="9.1" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#background-2"><i class="fa fa-check"></i><b>9.1</b> Background</a></li>
<li class="chapter" data-level="9.2" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#what-is-probability"><i class="fa fa-check"></i><b>9.2</b> What is probability?</a></li>
<li class="chapter" data-level="9.3" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#random-variables-probability"><i class="fa fa-check"></i><b>9.3</b> Random variables &amp; probability</a></li>
<li class="chapter" data-level="9.4" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#probability-and-the-bernoulli-distribution"><i class="fa fa-check"></i><b>9.4</b> Probability and the Bernoulli distribution</a></li>
<li class="chapter" data-level="9.5" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#probability-rules"><i class="fa fa-check"></i><b>9.5</b> Probability rules</a></li>
<li class="chapter" data-level="9.6" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#joint-probability"><i class="fa fa-check"></i><b>9.6</b> Joint probability</a></li>
<li class="chapter" data-level="9.7" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#conditional-probability"><i class="fa fa-check"></i><b>9.7</b> Conditional probability</a></li>
<li class="chapter" data-level="9.8" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#a-brief-note-on-likelihood-vs.-probability"><i class="fa fa-check"></i><b>9.8</b> A brief note on likelihood vs. probability</a></li>
<li class="chapter" data-level="9.9" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#probability-distributions-commonly-used-in-biological-statistics"><i class="fa fa-check"></i><b>9.9</b> Probability distributions commonly used in biological statistics</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#discrete-probability-distributions"><i class="fa fa-check"></i><b>9.9.1</b> Discrete Probability Distributions</a></li>
<li class="chapter" data-level="9.9.2" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#continuous-probability-distributions"><i class="fa fa-check"></i><b>9.9.2</b> <strong>Continuous probability distributions</strong></a></li>
</ul></li>
<li class="chapter" data-level="9.10" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#exercises-associated-with-this-chapter-1"><i class="fa fa-check"></i><b>9.10</b> Exercises associated with this chapter:</a></li>
<li class="chapter" data-level="9.11" data-path="introduction-to-probability-and-probability-distributions.html"><a href="introduction-to-probability-and-probability-distributions.html#additional-learning-resources-4"><i class="fa fa-check"></i><b>9.11</b> Additional learning resources:</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html"><i class="fa fa-check"></i><b>10</b> Correlation and Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#background-3"><i class="fa fa-check"></i><b>10.1</b> Background</a></li>
<li class="chapter" data-level="10.2" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#covariance-and-correlation"><i class="fa fa-check"></i><b>10.2</b> Covariance and correlation</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#covariance"><i class="fa fa-check"></i><b>10.2.1</b> Covariance</a></li>
<li class="chapter" data-level="10.2.2" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#correlation"><i class="fa fa-check"></i><b>10.2.2</b> Correlation</a></li>
<li class="chapter" data-level="10.2.3" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#hypothesis-tests-for-correlation"><i class="fa fa-check"></i><b>10.2.3</b> Hypothesis tests for correlation</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#hypothesis-tests-in-linear-regression"><i class="fa fa-check"></i><b>10.3.1</b> Hypothesis tests in linear regression</a></li>
<li class="chapter" data-level="10.3.2" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#linear-regression-in-r"><i class="fa fa-check"></i><b>10.3.2</b> Linear regression in <code>R</code></a></li>
<li class="chapter" data-level="10.3.3" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#a-note-on-the-coefficient-of-determination"><i class="fa fa-check"></i><b>10.3.3</b> A note on the coefficient of determination</a></li>
<li class="chapter" data-level="10.3.4" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#a-note-on-model-ii-regression"><i class="fa fa-check"></i><b>10.3.4</b> A note on model II regression</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="correlation-and-simple-linear-regression.html"><a href="correlation-and-simple-linear-regression.html#additional-learning-resources-5"><i class="fa fa-check"></i><b>10.4</b> Additional learning resources:</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html"><i class="fa fa-check"></i><b>11</b> Introduction to Analysis of Variance</a>
<ul>
<li class="chapter" data-level="11.1" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#background-4"><i class="fa fa-check"></i><b>11.1</b> Background</a></li>
<li class="chapter" data-level="11.2" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#general-linear-models"><i class="fa fa-check"></i><b>11.2</b> General linear models</a></li>
<li class="chapter" data-level="11.3" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#single-factor-anova"><i class="fa fa-check"></i><b>11.3</b> Single-factor ANOVA</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#single-factor-anova-hypothesis-tests"><i class="fa fa-check"></i><b>11.3.1</b> Single-factor ANOVA hypothesis tests</a></li>
<li class="chapter" data-level="11.3.2" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#anova-assumptions"><i class="fa fa-check"></i><b>11.3.2</b> ANOVA assumptions</a></li>
<li class="chapter" data-level="11.3.3" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#post-hoc-comparisons"><i class="fa fa-check"></i><b>11.3.3</b> Post-hoc comparisons</a></li>
<li class="chapter" data-level="11.3.4" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#single-factor-anova-in-r"><i class="fa fa-check"></i><b>11.3.4</b> Single-factor ANOVA in <code>R</code></a></li>
<li class="chapter" data-level="11.3.5" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#a-note-on-nonparametric-tests-similar-to-single-factor-anova"><i class="fa fa-check"></i><b>11.3.5</b> A note on nonparametric tests similar to single-factor ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#multi-factor-anova"><i class="fa fa-check"></i><b>11.4</b> Multi-factor ANOVA</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#nested-anova"><i class="fa fa-check"></i><b>11.4.1</b> Nested ANOVA</a></li>
<li class="chapter" data-level="11.4.2" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#factorial-anova"><i class="fa fa-check"></i><b>11.4.2</b> Factorial ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="introduction-to-analysis-of-variance.html"><a href="introduction-to-analysis-of-variance.html#additional-learning-resources-6"><i class="fa fa-check"></i><b>11.5</b> Additional learning resources:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Genetics Graduate Education Module - Spring 2021</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-probability-and-probability-distributions" class="section level1" number="9">
<h1><span class="header-section-number">Chapter 9</span> Introduction to Probability and Probability Distributions</h1>
<div id="background-2" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Background</h2>
<p>In this chapter, we will cover the basics of probability and common probability distributions. Probabilistic thinking can mark a significant departure in how we typically consider mathematics, and the world around us more generally. We have to shelve our natural inclination toward determinism, and embrace random variables, shades of likelihood, and complexity. As we’ll see, uncertainty in our estimates is a given. Indeed, the process of statistics is largely about quantifying and managing uncertainty - a process that begins with understanding probability distributions.</p>
<p>Frequently, we want to understand how likely a particular observation or set of observations is (e.g. from a sample of a population), given some expectation. That expectation may be based on a theoretical probability distribution we can use to model variation in nature. In this chapter we will introduce some core concepts of probability and how those pertain to understanding observed <strong>parameters</strong>, or features, and variation within systems.</p>
</div>
<div id="what-is-probability" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> What is probability?</h2>
<p>Statistical probability can be understood from two distinct perspectives: the “Frequentist” and the “Bayesian.”</p>
<ul>
<li><strong>Frequency interpretation</strong> <br>
<br>
“Probabilities are mathematically convenient approximations to long run relative frequencies.” <br>
<br></li>
<li><strong>Subjective (Bayesian) interpretation</strong> <br>
<br>
“Probability statements are expressions of the opinion of some individual or of current understanding regarding how certain an event is to occur.”
<br></li>
</ul>
<p>Both conceptions of probability are widely applied in data analysis, though most of the techniques discussed in this book are rooted in frequentist statistics.</p>
</div>
<div id="random-variables-probability" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Random variables &amp; probability</h2>
<p><br></p>
<p><strong>Probability</strong> is the expression of belief in some future outcome based on information about a system, and is typically applied in statistics to variables we want to understand or estimate in the real world. Specifically, a <strong>random variable</strong> can take on different values at varying probabilities based on its underlying sample space and probability distribution. The <strong>sample space</strong> of a random variable is the universe of all possible values for that variable. It may be helpful to think of the sample space in the form of a plotted function, where possible values of the random variable make up the x-axis, and the probability of “drawing” a particular value at random makes up the y-axis.</p>
<p><br></p>
<p>The <strong>sample space</strong> can be represented by a <strong>probability distribution</strong> when our random variable is discrete. By discrete we mean that the variable can take on a limited (finite) number of values. Meristic traits like the number of bristles on the abdomen of an insect or the number of action potentials a neuron experiences in a single window of time can only have positive integer values. Continuous random variables like human height, on the other hand, can in theory take on an infinite number of values, but are in practice limited by our measurement precision. For continuous variables, the sample space is represented by what we call a <strong>probability density function</strong> (PDF), also called a continuous probability distribution. Probabilities over a sample space <strong>always sum to 1.0</strong>, meaning that all possible values for that random variable are encompassed by its probability distribution, and we use tools from algebra (for probability distributions) and calculus (for probability density functions) to make use of their properties in statistical modeling and inference.</p>
<p><br></p>
<p>Distributions of random variables can be expressed as functions that have <strong>moments</strong>. These moments are metrics of a function’s shape, and these can be estimated. For example the 1st, 2nd, 3rd and 4th moments of a distribution correspond to the mean, variance, skewness, and kurtosis, respectively. For now let’s just consider the first two.</p>
<ul>
<li>The expectation or mean of a random variable X is:</li>
</ul>
<p><span class="math display">\[E[X] = \sum_{\text{all x}}^{}xP(X=x) = \mu\]</span>
<br></p>
<ul>
<li>Often we want to know how dispersed the random variable is around its mean</li>
<li>One measure of dispersion is the variance:</li>
</ul>
<p><span class="math display">\[Var(X) = E[X^2] = \sigma^2\]</span>
<br></p>
<p>There are many <strong>families</strong> or <strong>forms</strong> of probability distributions, and which ones we apply in statistics depend on the dynamical system we are trying to represent. We will return to the most commonly used ones below. Probability distributions are mathematically defined by features we call <em>parameters</em>, which correspond to the moments pointed out above. The parameters of the functions themselves are used to understand properties of the systems we use the functions to model. For example the normal distribution (also called the Gaussian distribution, depicted by a bell curve), which is probably the most famous distribution in statistics, is characterized by 2 parameters: <span class="math inline">\(mu\)</span> (the mean) and <span class="math inline">\(sigma^{2}\)</span> (the variance). In practical terms, those parameters dictate the central peak or “mode” and the spread (width), respectively.</p>
<p>These parameters are clearly important for us in thinking about the systems we study. For example in biology we often think about random variables as values expressed by individual living things. We may consider, in theory, all possible individuals under a given set of circumstances, and one or more random variables associated with those individuals. In statistics we call this theoretical notion of all individuals a <strong><em>population</em></strong>. If we can assume that a random variable in that population has a particular probability distribution, it opens the door to estimating the aforementioned population parameters from a <strong><em>random sample</em></strong> of that population. Mean height definitely tells us something about the most common values in a population of humans, as does the variability of height among individuals. So you can see how probability distributions, when applied under the appropriate assumptions, help us understand, quantify, and compare random variables in populations. We will further explore how population parameters are estimated from random samples in the next chapter. For now, we will introduce various probability distributions and the random variables they represent.</p>
</div>
<div id="probability-and-the-bernoulli-distribution" class="section level2" number="9.4">
<h2><span class="header-section-number">9.4</span> Probability and the Bernoulli distribution</h2>
<p>To think about probability and probability distributions, let’s start with the Bernoulli distribution. It describes the expected outcome of an event with probability <code>p</code>. A simple example of this scenario is the flipping of a coin. If that coin is <strong>fair</strong>, then the probabilities of heads or tails are</p>
<p><br></p>
<p><span class="math display">\[Pr(X=\text{Head}) = \frac{1}{2} = 0.5 = p \]</span></p>
<p><span class="math display">\[Pr(X=\text{Tails}) = \frac{1}{2} = 0.5 = 1 - p \]</span></p>
<p>If the coin isn’t fair then <span class="math inline">\(p \neq 0.5\)</span>. At this point, we don’t know whether our coin is fair or not, so let’s estimate the Bernoulli distribution of our coin flip by flipping our coin 1000 times and visualize the results.</p>
<p><img src="quantitative_genetics_GEM_files/figure-html/unnamed-chunk-48-1.png" width="288" style="display: block; margin: auto;" /></p>
<p>We can see that our estimated Bernoulli distribution indicates a coin that is very close to fair (see Chapter 13: Frequency Analysis for how we might test this statistically). Note that the probabilities still sum to 1, a property of all probability distributions.
<br>
<br>
<span class="math display">\[ p + (1-p) = 1 \]</span>
<br>
<br>
The Bernoulli distribution can be used to represent other binary possibilities, like success or failure, “yes” or “no” answers, choosing an allele at a biallelic locus from a population, etc…</p>
</div>
<div id="probability-rules" class="section level2" number="9.5">
<h2><span class="header-section-number">9.5</span> Probability rules</h2>
<p>Let’s take a moment to cover some basic rules of probability regarding the observation of multiple “events.”</p>
<p>Let’s say we flip a fair coin twice. Represent the first flip as ‘X’ and the second flip as ‘Y.’ <code>H</code> indicates a Heads and <code>T</code> a Tails. The probability for any given sequence of both flips is</p>
<p><br>
<br></p>
<p><span class="math display">\[ Pr(\text{X=H and Y=H}) = p*p = p^2 \]</span>
<span class="math display">\[ Pr(\text{X=H and Y=T}) = p*p = p^2 \]</span>
<span class="math display">\[ Pr(\text{X=T and Y=H}) = p*p = p^2 \]</span>
<span class="math display">\[ Pr(\text{X=T and Y=T}) = p*p = p^2 \]</span></p>
<p><br></p>
<p>While the probability of flipping both an <code>H</code> and <code>T</code> in any order is</p>
<p><br></p>
<p><span class="math display">\[ \text{Pr(X=H and Y=T) or Pr(X=T and Y=H)} = \]</span>
<span class="math display">\[ (p*p) + (p*p) = 2p^{2} \]</span>
<br>
<br></p>
<p>These are the <strong>‘and’</strong> and <strong>‘or’</strong> rules for the probability of multiple events:</p>
<ul>
<li>‘and’ means multiply the probabilities to attain the probability of both events happening</li>
<li>‘or’ means sum the probabilities to attain the probability of either event happening</li>
<li>most probability distributions can be built up from these simple rules</li>
</ul>
</div>
<div id="joint-probability" class="section level2" number="9.6">
<h2><span class="header-section-number">9.6</span> Joint probability</h2>
<p>The joint probability is the probability of two or more outcomes co-occurring. Following the ‘and’ rule,</p>
<p><br>
<br>
<span class="math display">\[Pr(X,Y) = Pr(X) * Pr(Y)\]</span><br />
<br></p>
<p>As above, this multiplication is true for two <strong>independent</strong> events. However, for two non-independent events we also have to take into account their <strong>covariance</strong>. To do this we need to determine their <strong>conditional probabilities</strong>.</p>
</div>
<div id="conditional-probability" class="section level2" number="9.7">
<h2><span class="header-section-number">9.7</span> Conditional probability</h2>
<p>Variables that are non-independent have a shared variance, which is also known as <strong>covariance</strong>. You can think of this as two variables that consistently deviate from their respective means. Covariance standardized to a mean of zero and a unit standard deviation is <strong>correlation</strong>, which we’ll discuss in detail in Chapter 12. To assess the probability of two events where they might not be independent, we must considering their conditional probability.</p>
<p><br></p>
<ul>
<li>The conditional probability for two <strong>independent</strong> variables:</li>
</ul>
<p><br></p>
<p><span class="math display">\[Pr(Y|X) = Pr(Y)\]</span>
<span class="math display">\[Pr(X|Y) = Pr(X)\]</span>
<br></p>
<p>This means that the probability of <code>Y</code> given <code>X</code> is just the probability of <code>Y</code>, and the reverse is true for the probability of <code>X</code> given <code>Y</code>. In other words, the occurrence of event <code>X</code> or <code>Y</code> has no influence on the occurrence of the other event. These variables are therefore independent.</p>
<p><br></p>
<ul>
<li>The conditional probability for two <strong>non-independent</strong> variables:</li>
</ul>
<p><br></p>
<p><span class="math display">\[Pr(Y|X) \neq Pr(Y)\]</span>
<span class="math display">\[Pr(X|Y) \neq Pr(X)\]</span>
<br></p>
<p>In this case, the probability of <code>Y</code> given <code>X</code> <em>does not</em> equal the probability of just <code>Y</code>. Thus, one is influencing the probability of the other. More specifically, when we have two non-independent events, the equation for the conditional probability of one event given the other is</p>
<p><br></p>
<p><span class="math display">\[Pr(Y|X) = \frac{Pr(X|Y)Pr(Y)}{Pr(X)}\]</span>
<br></p>
<p>which is also known as <strong>Bayes’ Theorem</strong>.</p>
</div>
<div id="a-brief-note-on-likelihood-vs.-probability" class="section level2" number="9.8">
<h2><span class="header-section-number">9.8</span> A brief note on likelihood vs. probability</h2>
<ul>
<li><p>The <strong>probability</strong> of an event is the proportion of times that the event would occur if we repeated a random trial over and over again under the same conditions.</p></li>
<li><p>The <strong>likelihood</strong> is the probability of observing a particular set of data or outcome, given a particular parameter value.</p></li>
</ul>
<p><code>L[parameter|data] = Pr[data|parameter]</code></p>
<p>Extending from this, the parameter value at which the likelihood is maximized is called the maximum likelihood estimate (MLE). You don’t need to worry too much about likelihood in this course, but realize that many of our formualae for estimating parameters from data actually produce maximum likelihood estimates. The formula we use to calculate a mean from a sample of observations, for example, produces the maximum likelihood estimate for the population mean from which that sample was taken. The <strong>likelihood function</strong> (for a single parameter) or <strong>likelihood surface</strong> (for multiple parameters) describes the relationship between different parameter values and their likelihood. We can’t always derive convenient equations to obtain maximum likelihood estimates, however, and in those cases we may have to rely on algorithmic searches of “parameter space” to find the MLE.</p>
</div>
<div id="probability-distributions-commonly-used-in-biological-statistics" class="section level2" number="9.9">
<h2><span class="header-section-number">9.9</span> Probability distributions commonly used in biological statistics</h2>
<p>(Many of these are thanks to Sally Otto at UBC)</p>
<div id="discrete-probability-distributions" class="section level3" number="9.9.1">
<h3><span class="header-section-number">9.9.1</span> Discrete Probability Distributions</h3>
<div id="geometric-distribution" class="section level4" number="9.9.1.1">
<h4><span class="header-section-number">9.9.1.1</span> <strong>Geometric Distribution</strong></h4>
<p>If a single event has two possible outcomes at probability <code>p</code> and <code>1-p</code>, and is independent of past events (<em>i.e.</em> a Bernoulli trial), the probability of having to observe <code>k</code> trials before the first “success” appears is given by the <strong>geometric distribution</strong>. The probability that the first “success” would appear on the first trial is <code>p</code>, but the probability that the <em>first</em> “success” appears on the second trial is <code>(1-p)*p</code>. By generalizing this procedure, the probability that there will be <code>k-1</code> failures before the first success is:</p>
<p><span class="math display">\[P(X=k)=(1-p)^{k-1}p\]</span></p>
<ul>
<li>mean = <span class="math inline">\(\frac{1}{p}\)</span></li>
<li>variance = <span class="math inline">\(\frac{(1-p)}{p^2}\)</span></li>
</ul>
<div id="the-geometric-distribution-in-practice" class="section level5" number="9.9.1.1.1">
<h5><span class="header-section-number">9.9.1.1.1</span> The <strong>Geometric Distribution</strong> in practice</h5>
<p>The geometric distribution applies in any scenario in which we want to know the probability of a certain number of failures before we observe an event (assuming each trial is independent). Dice rolls, free throws in basketball, sales pitches, and many more such sequential trials with two outcomes are modeled well by the geometric distribution.</p>
<p>For example, if the probability of extinction of an endangered population is estimated to be 0.1 every year, what is the expected time until extinction?</p>
<p><img src="images/prob.017.jpeg" width="80%" style="display: block; margin: auto;" /></p>
<p>Here we can see the probability of extinction in any given year. If we wanted to know the probability of extinction <em>by</em> a specific year, we can simply apply the ‘or’ rule and sum the probabilities up until the specified year. The probability of extinction by year 4 is equivalent to the probability of extinction in year 1 <em>or</em> year 2 <em>or</em> year 3, <span class="math inline">\(0.1 + (1 - 0.1)*0.1 + (1 - 0.1)^2*0.1 = 0.271\)</span>.</p>
</div>
</div>
<div id="binomial-distribution" class="section level4" number="9.9.1.2">
<h4><span class="header-section-number">9.9.1.2</span> <strong>Binomial Distribution</strong></h4>
<p>A <strong>binomial distribution</strong> represents the distribution of outcomes from the <strong>combination</strong> of several Bernoulli trials <em>i.e.</em> independent trials with only two outcomes. In fact, the Bernoulli distribution is just a special case of the binomial distribution for n = 1 Bernoulli trials. The distribution of probabilities for each combination of outcomes is</p>
<p><span class="math display">\[\large f(k) = {n \choose k} p^{k} (1-p)^{n-k}\]</span>
- <code>n</code> is the total number of trials
- <code>k</code> is the number of successes
- <code>p</code> is the probability of success
- <code>q</code> is the probability of not success
- For binomial as with the Bernoulli <code>p = 1-q</code></p>
<div id="the-binomial-distribution-in-practice" class="section level5" number="9.9.1.2.1">
<h5><span class="header-section-number">9.9.1.2.1</span> The <strong>binomial distribution</strong> in practice</h5>
<p>Pretend that you flip 20 fair coins (or collect alleles from a heterozygote). Now repeat that process 100 times and record the number of heads that show. We expect that most of the time we will get approximately 10 heads in 20 flips. However, sometimes we will get many fewer heads or many more heads. If we plot the frequency of the proportion of “successes,” or heads, we get in each of our 100 replicates, we get the binomial distribution. Because our coin is fair, we can reasonably expect this distribution to center around 0.5.</p>
<p><img src="images/week_2.003.jpeg" width="100%" style="display: block; margin: auto;" /></p>
<p>The binomial distribution is the basis for frequency tests when outcomes are binary.</p>
</div>
</div>
<div id="negative-binomial-distribution" class="section level4" number="9.9.1.3">
<h4><span class="header-section-number">9.9.1.3</span> <strong>Negative Binomial Distribution</strong></h4>
<p>The <strong>negative binomial distribution</strong> is an extension of the geometric distribution describing the expected time until not just one success but <code>r</code> “successes” have occurred. Mathematically, it is a generalization of the geometric distribution, where the probability of the <span class="math inline">\(r^{th}\)</span> “success” appearing on the <span class="math inline">\(k^{th}\)</span> trial is:</p>
<p><span class="math display">\[P(X=k)=(\frac{k-1}{r-1})p^{r-1}(1-p)^{k-r}p\]</span></p>
<p><br></p>
<p>which simplifies to</p>
<p><span class="math display">\[P(X=k)=(\frac{k-1}{r-1})p^{r}(1-p)^{k-r}\]</span></p>
<ul>
<li>mean = <span class="math inline">\(\frac{r}{p}\)</span></li>
<li>variance = <span class="math inline">\(r(1-p)/p^2\)</span></li>
</ul>
<p>For example, if a predator must capture 10 prey before it can grow large enough to reproduce, what would be the expected age of onset of reproduction if the probability of capturing a prey on any given day is 0.1?</p>
<p><img src="images/prob.018.jpeg" width="50%" style="display: block; margin: auto;" /></p>
<p>Notice that the variance is quite high (~1000) and the distribution is fairly skewed. Generally, a low probability of success <code>p</code> and a high threshold of successes <code>r</code> leads to a highly dispersed distribution with considerable kurtosis (‘tailedness’).</p>
</div>
<div id="poisson-probability-distribution" class="section level4" number="9.9.1.4">
<h4><span class="header-section-number">9.9.1.4</span> <strong>Poisson Probability Distribution</strong></h4>
<p>Another common situation in biology is when each trial is discrete, but the number of observations of each outcome is observed/counted. Such scenarios are modeled well by the <strong>Poisson distribution</strong>. For example, counts of snails in several plots of land, observations of the firing of a neuron in a unit of time, or count of genes in a genome binned to units of 500 AA. Just like before, you have ‘successes,’ but now you count them for each replicate where replicates are now units of area or time. Values can now range from 0 to a large number.</p>
<p>For example, you can examine 1000 genes and count the number of base pairs in the coding region of each gene. What is the probability of observing a gene with ‘r’ bp?</p>
<p><code>Pr(Y=r)</code> is the probability that the number of occurrences of an event <code>y</code> equals a count <code>r</code> in the total number of trials.</p>
<p><br></p>
<p><span class="math display">\[Pr(Y=r) = \frac{e^{-\mu}\mu^r}{r!}\]</span></p>
<p>Note that this is a single parameter function because <span class="math inline">\(\mu = \sigma^2\)</span> - the two together are often just represented by <span class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[Pr(y=r) = \frac{e^{-\lambda}\lambda^r}{r!}\]</span></p>
<p>This means that for a variable that is truly Poisson distributed, the mean and variance should be roughly equal to one another. Variables that are approximately Poisson distributed but have a larger variance than the mean are called ‘overdispersed,’ indicating that the observed variance is larger than appropriate for the theoretical distribution. This is quite common in RNA-seq and microbiome data. When overdispersion is a problem in count data, we often use the negative binomial distribution instead because it allows the variance to differ from the mean.</p>
<div id="poisson-probability-distribution-gene-length-by-bins-of-500-nucleotides" class="section level5" number="9.9.1.4.1">
<h5><span class="header-section-number">9.9.1.4.1</span> Poisson Probability Distribution | gene length by bins of 500 nucleotides</h5>
<p><img src="images/week_2.004.jpeg" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="poisson-probability-distribution-increasing-parameter-values-of-lambda" class="section level5" number="9.9.1.4.2">
<h5><span class="header-section-number">9.9.1.4.2</span> Poisson Probability Distribution | increasing parameter values of <span class="math inline">\(\lambda\)</span></h5>
<p><img src="images/week_2.005.jpeg" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="horse-kick-deaths-in-the-prussian-army" class="section level5" number="9.9.1.4.3">
<h5><span class="header-section-number">9.9.1.4.3</span> Horse kick deaths in the Prussian army</h5>
<p>One of the earliest applications of the Poisson distribution was in 1898, when it was used to model the number of soldier deaths from horse kicks in 14 different corps of the Prussian army. As can be seen from the chart below, the Poisson distribution does a remarkable job at modeling these unfortunate events. Indeed, while it is useful for count data in general, it is particularly effective at modeling the distribution of unlikely, independent events.</p>
<p><img src="quantitative_genetics_GEM_files/figure-html/unnamed-chunk-54-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="continuous-probability-distributions" class="section level3" number="9.9.2">
<h3><span class="header-section-number">9.9.2</span> <strong>Continuous probability distributions</strong></h3>
<p>Up until this point, we have been looking at <em>discrete</em> probability distributions, where our measurements represent integer or categorical values (event outcomes, counts, etc…) and the probability of a specific observation can be directly quantified. We will now discuss probability density functions (PDFs), better known as <strong>continuous probability distributions</strong>. These represent the distribution of <em>continuous</em> values, from which a random sample can take on an infinite number of values within the range of the distribution (limited by measurement accuracy). As such, unlike discrete probability distributions, the probability of finding any <em>exact</em> value within a continuous distribution is effectively 0. We must instead look at the probability of a measurement falling between a range of values, <code>a</code> and <code>b</code> <em>i.e.</em> the integral of the density function (the area beneath the curve) between said values.</p>
<p><br></p>
<p>P(observation lies within dx of x) = f(x)dx</p>
<p><span class="math display">\[P(a\leq X \leq b) = \int_{a}^{b} f(x) dx\]</span></p>
<p><br></p>
<p>Remember that the indefinite integral sums to one</p>
<p><span class="math display">\[\int_{-\infty}^{\infty} f(x) dx = 1\]</span></p>
<p><br></p>
<p>The expected value of a random variable <code>X</code>, <code>E[X]</code>, may be found by integrating the product of <code>x</code> and the probability density function over all possible values of <code>x</code>:</p>
<p><span class="math display">\[E[X] = \int_{-\infty}^{\infty} xf(x) dx \]</span></p>
<p><br></p>
<p><span class="math inline">\(Var(X) = E[X^2] - (E[X])^2\)</span>, where the expectation of <span class="math inline">\(X^2\)</span> is</p>
<p><span class="math display">\[E[X^2] = \int_{-\infty}^{\infty} x^2f(x) dx \]</span></p>
<div id="uniform-distribution" class="section level4" number="9.9.2.1">
<h4><span class="header-section-number">9.9.2.1</span> <strong>Uniform Distribution</strong></h4>
<p>The uniform distribution is rectangular, meaning that all values have equal probability between the bounds of the distribution <span class="math inline">\([a,b]\)</span>. Its PDF for an expected value of <code>X</code> is given by</p>
<p><br></p>
<p><span class="math display">\[E[X] = \int_{a}^{b} x\frac{1}{b-a} dx = \frac{(a+b)}{2} \]</span></p>
<p><br></p>
<p><img src="images/prob.019.jpeg" width="100%" style="display: block; margin: auto;" /></p>
<p>While we are introducing it here as a continuous probability distribution, the uniform distribution has a discrete application as well. Both are used to describe any scenario in which an outcome has equal probability to occur, like true random number generation, or a dice roll in the discrete case.</p>
</div>
<div id="exponential-distribution" class="section level4" number="9.9.2.2">
<h4><span class="header-section-number">9.9.2.2</span> <strong>Exponential Distribution</strong></h4>
<p>The <strong>exponential distribution</strong> can be thought of as the continuous alternative to the geometric distribution, describing the probability of the occurrence of an event or state change over time, given a continuous process. It is defined by a single parameter, the rate constant <span class="math inline">\(\lambda\)</span>, which represents the instantaneous probability of an event occurring. The PDF is</p>
<p><br></p>
<p><span class="math display">\[f(x)=\lambda e^{-\lambda x}\]</span></p>
<p><code>E[X]</code> can be found be integrating <span class="math inline">\(xf(x)\)</span> from 0 to infinity, leading to the result that</p>
<p><br></p>
<ul>
<li><span class="math inline">\(E[X] = \frac{1}{\lambda}\)</span></li>
<li><span class="math inline">\(E[X^2] = \frac{1}{\lambda^2}\)</span></li>
</ul>
<p>For example, let <span class="math inline">\(\lambda\)</span> represent the instantaneous death rate of an individual. The expected lifespan of that individual would be described by an exponential distribution (assuming that <span class="math inline">\(\lambda\)</span> does not change over time).</p>
<p><img src="images/prob.020.jpeg" width="70%" style="display: block; margin: auto;" /></p>
<p>More generally, the exponential distribution describes many situations in which the probability of an event is approximately constant and independent. It is widely applied in survival analysis, actuarial sciences, marketing, and the physical sciences (particularly any process exhibiting exponential decay).</p>
</div>
<div id="gamma-distribution" class="section level4" number="9.9.2.3">
<h4><span class="header-section-number">9.9.2.3</span> <strong>Gamma Distribution</strong></h4>
<p>The gamma distribution generalizes the exponential distribution in the same way that the negative binomial distribution generalizes the geometric distribution. Instead of representing the probability of the first occurrence of an event, it models the waiting time until the <span class="math inline">\(r^{th}\)</span> event for a process that occurs randomly over time at a rate <span class="math inline">\(\lambda\)</span>:</p>
<p><br></p>
<p><span class="math display">\[f(x) = \frac{e^{-\lambda x}\lambda x^{r-1}}{(r-1)!}\lambda\]</span></p>
<p><br></p>
<p><span class="math display">\[ Mean =  \frac{r}{\lambda} \]</span>
<span class="math display">\[ Variance = \frac{r}{\lambda^2} \]</span></p>
<p><br></p>
<p>For example, if in a PCR reaction, DNA polymerase synthesizes new DNA strands at a rate of 1 per millisecond, how long until 1000 new DNA strands are produced? Assume that DNA synthesis does not deplete the pool of primers or nucleotides in the chamber, so that each event is independent of other events in the PCR chamber.</p>
</div>
<div id="the-gaussian-or-normal-distribution" class="section level4" number="9.9.2.4">
<h4><span class="header-section-number">9.9.2.4</span> The Gaussian or Normal Distribution</h4>
<p>The ‘Gaussian,’ or <strong>Normal distribution</strong> is one of the best known probability distributions. Many people whether statistically versed or not have an intuitive understanding of the normal distribution because it models the nature of random continuous variables in a population well - that is, they have a central tendency plus a constrained amount of deviation around this tendency (a “bell curve”). The normal distribution has two parameters, the mean <span class="math inline">\(\mu\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span>. The PDF of the normal distribution is defined as</p>
<p><img src="images/week_2.032.jpeg" width="40%" style="display: block; margin: auto;" />
where
<span class="math display">\[\large \pi \approx 3.14159\]</span></p>
<p><span class="math display">\[\large e \approx 2.71828\]</span></p>
<p>To write that a variable (v) is distributed as a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, we write the following:</p>
<p><span class="math display">\[\large v \sim \mathcal{N} (\mu,\sigma^2)\]</span></p>
<div id="normal-pdf-estimates-of-mean-and-variance" class="section level5 smaller" number="9.9.2.4.1">
<h5><span class="header-section-number">9.9.2.4.1</span> Normal PDF | estimates of mean and variance</h5>
<p>Estimate of the mean from a single sample</p>
<p><span class="math display">\[\Large \bar{x} = \frac{1}{n}\sum_{i=1}^{n}{x_i} \]</span></p>
<p>Estimate of the variance from a single sample</p>
<p><span class="math display">\[\Large s^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(x_i - \bar{x})^2} \]</span></p>
<p><img src="images/week_2.010.jpeg" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="why-is-the-normal-distribution-special-in-biology" class="section level5" number="9.9.2.4.2">
<h5><span class="header-section-number">9.9.2.4.2</span> Why is the Normal distribution special in biology?</h5>
<p><img src="images/week_2.013.jpeg" width="100%" style="display: block; margin: auto;" /></p>
<p><img src="images/week_2.015.jpeg" width="100%" style="display: block; margin: auto;" /></p>
<p><img src="images/week_2.014.jpeg" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="parent-offspring-resemblance" class="section level5 smaller" number="9.9.2.4.3">
<h5><span class="header-section-number">9.9.2.4.3</span> Parent-offspring resemblance</h5>
<p><img src="images/week_2.016.jpeg" width="45%" style="display: block; margin: auto;" /></p>
</div>
<div id="genetic-model-of-complex-traits" class="section level5 smaller" number="9.9.2.4.4">
<h5><span class="header-section-number">9.9.2.4.4</span> Genetic model of complex traits</h5>
<p><img src="images/week_2.017.jpeg" width="90%" style="display: block; margin: auto;" /></p>
</div>
<div id="distribution-of-f_2-genotypes-really-just-binomial-sampling" class="section level5 smaller" number="9.9.2.4.5">
<h5><span class="header-section-number">9.9.2.4.5</span> Distribution of <span class="math inline">\(F_2\)</span> genotypes | really just binomial sampling</h5>
<p><img src="images/week_2.018.jpeg" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="the-central-limit-theorm" class="section level5" number="9.9.2.4.6">
<h5><span class="header-section-number">9.9.2.4.6</span> The Central Limit Theorm</h5>
<p>The normal distribution is immensely useful because of the <strong>Central Limit Theorem</strong>, which says that the mean or the sum of many random variables independently drawn from the same distribution is distributed approximately normally. One can think of numerous real-world situations in which this applies, such as when multiple genes contribute to a phenotype or when many factors contribute to a biological process. In addition, whenever there is variance introduced by stochastic factors the central limit theorem holds. Thus, normal distributions occur throughout genomics, while representing the basis of classical statistics.</p>
</div>
<div id="a-note-on-z-scores-of-normal-variables" class="section level5" number="9.9.2.4.7">
<h5><span class="header-section-number">9.9.2.4.7</span> A note on z-scores of normal variables</h5>
<p>Often we want to make variables more directly comparable to one another, particularly when they have scales differing by one or more orders of magnitude. For example, consider measuring the leg length of mice and of elephants. Which animal has longer legs in absolute terms? What about proportional to their body size? A good way to answer these last questions is to use ‘z-scores.’</p>
<p>A z-score is a statistic standardized to a mean of 0 and a standard deviation of 1. To attain the z-score for a given estimate, we can modify any normal distribution to have a mean of 0 and a standard deviation of 1 by normalizing the distribution to the population’s standard deviation (another term for this is the standard normal distribution). The z-score, then, is the number of standard deviations from the mean (0) of this distribution.</p>
<p><br></p>
<p><span class="math display">\[\huge z_i = \frac{(x_i - \bar{x})}{s}\]</span></p>
</div>
</div>
</div>
</div>
<div id="exercises-associated-with-this-chapter-1" class="section level2" number="9.10">
<h2><span class="header-section-number">9.10</span> Exercises associated with this chapter:</h2>
<ul>
<li>Problem Set 2</li>
</ul>
</div>
<div id="additional-learning-resources-4" class="section level2" number="9.11">
<h2><span class="header-section-number">9.11</span> Additional learning resources:</h2>
<ul>
<li><p>Irizarry, R. A. Introduction to Data Science. <a href="https://rafalab.github.io/dsbook/" class="uri">https://rafalab.github.io/dsbook/</a> - A gitbook written by a statistician, with great introductions to key topics in statistical inference.</p></li>
<li><p>Logan, M. 2010. Biostatistical Design and Analysis Using R. - A great intro to R for statistical analysis</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="a-brief-introduction-to-rmarkdown.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="correlation-and-simple-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-probability_probdists.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["quantitative_genetics_GEM.pdf", "quantitative_genetics_GEM.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
